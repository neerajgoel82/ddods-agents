# AI Agents Crash Course - Part 10: Complete Summary

**Implementing ReAct Agentic Pattern From Scratch**

**Authors:** Avi Chawla & Akshay Pachaar  
**Source:** Daily Dose of Data Science  
**Date:** April 13, 2025

## üéØ Executive Summary

Part 10 provides a deep technical dive into implementing the **ReAct (Reasoning and Acting) pattern** from scratch using pure Python and LLMs, without relying on high-level frameworks like CrewAI or LangChain. This article bridges the gap between using pre-built frameworks and understanding the underlying mechanics that power agentic systems.

---

## üÜï New Concepts Beyond Parts 1-9

### **Understanding Framework Internals**
While previous parts focused on using CrewAI as a framework, Part 10 reveals the **underlying implementation details** that make agentic systems work:

- **Prompt Engineering Architecture**: How structured prompts create agent behavior
- **Tool Invocation Mechanics**: The technical process of tool calling and result processing
- **ReAct Loop Implementation**: Manual step-by-step execution vs. automated orchestration
- **LLM Integration Patterns**: Direct API integration without framework abstractions

### **The ReAct Pattern Deep Dive**

**Core Loop Structure:**
```
Thought ‚Üí Action ‚Üí Observation ‚Üí [Repeat] ‚Üí Answer
```

**Key Innovation:** ReAct agents don't generate direct answers in one step. Instead, they:
- **Think step-by-step** about what to do next
- **Perform intermediate actions** (tool calls, calculations)
- **Observe results** before continuing
- **Chain multiple iterations** until reaching a final answer

---

## üèóÔ∏è Technical Implementation Architecture

### **1. Basic Agent Class Structure**

**Core Components:**
```python
class MyAgent:
    def __init__(self, system=""):
        self.system = system          # System prompt
        self.messages = []           # Conversation history
    
    def complete(self, message=""):
        # Records user input
        # Calls LLM via completion API
        # Updates conversation history
        # Returns LLM response
```

**Key Features:**
- **Conversation Memory**: Maintains full message history across interactions
- **System Prompt Integration**: Behavioral constraints defined via system messages
- **LLM Abstraction**: Uses LiteLLM for provider-agnostic LLM calls

### **2. ReAct System Prompt Design**

**Behavioral Protocol Structure:**
```
You run in a loop and do JUST ONE thing in a single iteration:
1) "Thought" to describe your thoughts about the input question
2) "PAUSE" to pause and think about the action to take
3) "Action" to decide what action to take from available tools
4) "PAUSE" to pause and wait for the result of the action
5) "Observation" will be the output returned by the action
```

**Critical Design Elements:**
- **Explicit Loop Control**: Forces step-by-step execution rather than rushing to answers
- **Tool API Reference**: Detailed specifications for each available tool
- **Output Format Constraints**: Structured response format with clear terminators
- **Error Prevention**: Constraints against hallucinating non-existent tools

### **3. Tool Integration Mechanics**

**Tool Definition Pattern:**
```python
def math(expression: str):
    return eval(expression)

def lookup_population(country: str):
    populations = {
        "India": 1_400_000_000,
        "Japan": 125_000_000,
        # ... other countries
    }
    return populations.get(country, "Country not found")
```

**Tool Execution Process:**
1. **LLM generates structured action**: `Action: lookup_population: India`
2. **Regex parsing extracts**: Tool name and arguments
3. **Python function call**: Execute tool with parsed arguments
4. **Result formatting**: `Observation: {result}` injected back to LLM
5. **Context continuation**: LLM processes result and continues reasoning

---

## üîÑ Implementation Approaches

### **Manual Execution (Educational)**

**Step-by-Step Process:**
1. **Initialize agent** with ReAct system prompt
2. **Send user query** to start the reasoning process
3. **Manually call `complete()`** after each step
4. **Parse LLM responses** for Thought/Action/Observation patterns
5. **Manually inject tool results** as observations
6. **Continue until** LLM produces final answer

**Benefits:**
- **Complete transparency** into each reasoning step
- **Full control** over tool execution and result validation
- **Educational insight** into ReAct mechanics
- **Debugging capabilities** at each iteration

### **Automated Execution (Production-Ready)**

**Controller Function Architecture:**
```python
def agent_loop(query, system_prompt: str = ""):
    # Initialize agent and tools
    # Track conversation state
    # Parse LLM responses automatically
    # Execute tools programmatically
    # Inject observations seamlessly
    # Return final answer
```

**Key Features:**
- **Automatic parsing** of LLM responses using regex patterns
- **Dynamic tool execution** based on LLM requests
- **Error handling** for invalid tool calls
- **Loop termination** detection via "Answer:" keyword
- **State management** across multiple iterations

---

## üéØ Advanced Technical Insights

### **Prompt Engineering for Control**

**ReAct Format Template:**
```
IMPORTANT: Use the following format in your response:

Thought: you should always think about what to do
Action: the action to take, only one name of [Tool Name], just the name
Action Input: the input to the action, simple JSON object
Observation: the result of the action

Once all necessary information is gathered, return:
Thought: I now know the final answer
Final Answer: the final answer to the original input question
```

**Design Principles:**
- **Structured Output**: Forces consistent formatting for automated parsing
- **Clear Termination**: Explicit "Final Answer" signals completion
- **Tool Constraints**: Prevents hallucination of non-existent capabilities
- **JSON Input Format**: Enables reliable parameter extraction

### **Tool Call Processing Pipeline**

**Pattern Recognition:**
```python
# Regex pattern for action parsing
pattern = r"Action:\s*(\w+):\s*(.+)"
match = re.search(pattern, llm_response)

if match:
    chosen_tool = match.group(1)  # Tool name
    arg = match.group(2)          # Tool argument
    
    # Execute tool if available
    if chosen_tool in available_tools:
        observation = available_tools[chosen_tool](arg)
        current_prompt = f"Observation: {observation}"
```

**Error Handling:**
- **Invalid tool detection**: "Tool not available. Retry the action."
- **Tool execution errors**: Graceful error message injection
- **Format validation**: Ensures proper action syntax

---

## üìã Production Considerations

### **Limitations of Basic Implementation**

**Current Approach Issues:**
- **Regex brittleness**: Small format deviations can break parsing
- **No tool validation**: Assumes all tool calls will succeed
- **Limited error recovery**: Basic retry mechanisms only
- **Hardcoded tool mapping**: Tools must be explicitly registered

**Production Enhancements Needed:**
- **Structured prompts with JSON outputs** or function calling
- **Tool validation, retries, and exception handling**
- **Guardrails or output formatters** to constrain LLM behavior
- **More robust parsing** (e.g., structured prompts with JSON outputs)

### **Scaling Considerations**

**Performance Optimizations:**
- **Async execution** for multiple concurrent agents
- **Tool result caching** to avoid redundant computations
- **Conversation history management** to prevent context overflow
- **Dynamic tool loading** for extensible capabilities

---

## üîç Key Differences from Framework Usage

| Aspect | Framework (CrewAI) | From Scratch |
|--------|-------------------|--------------|
| **Setup Complexity** | High-level abstractions | Manual implementation |
| **Control Level** | Framework-managed | Complete developer control |
| **Debugging** | Limited visibility | Full transparency |
| **Customization** | Framework constraints | Unlimited flexibility |
| **Production Ready** | Built-in robustness | Requires custom hardening |
| **Learning Value** | Black box usage | Deep understanding |

---

## üéØ Key Takeaways

### **‚úÖ Understanding ReAct Internals**
1. **ReAct is prompt engineering**: No magical built-in LLM ability, just carefully crafted instructions
2. **Structured templates are critical**: Consistent formatting enables automated parsing
3. **Tool integration is straightforward**: Simple function calls with result injection
4. **State management matters**: Conversation history enables multi-step reasoning

### **‚úÖ Implementation Insights**
1. **Manual execution for learning**: Step-by-step control reveals agent decision-making
2. **Automated loops for production**: Programmatic parsing enables scalable deployment
3. **Error handling is essential**: Robust systems require comprehensive failure management
4. **Flexibility vs. robustness trade-off**: Custom implementations offer control but require hardening

### **‚úÖ Production Path Forward**
1. **Start with manual implementation** for deep understanding
2. **Add automation layer** for operational efficiency
3. **Implement robust error handling** for reliability
4. **Consider framework adoption** for complex production requirements

---

## üöÄ Foundation for Advanced Patterns

**This implementation provides groundwork for:**
- **Research agents**: Multi-step information gathering and synthesis
- **Data wranglers**: Complex data processing and transformation workflows
- **Local retrieval systems**: Knowledge-based question answering
- **Autonomous assistants**: Goal-oriented task completion

**Next Steps:**
- **Enhanced tool ecosystems**: Custom domain-specific capabilities
- **Multi-agent coordination**: Distributed reasoning and collaboration
- **Memory integration**: Persistent context across sessions
- **Advanced prompt patterns**: More sophisticated reasoning frameworks

---

## üéâ Conclusion

Part 10 provides crucial insight into the mechanics underlying agentic frameworks by implementing ReAct from scratch. The key revelation is that sophisticated agent behavior emerges from carefully designed prompts and structured execution loops rather than built-in LLM capabilities.

This foundation enables developers to build custom agentic solutions with complete control over behavior, debugging, and optimization while understanding exactly how frameworks like CrewAI operate under the hood.

**Bottom Line:** Understanding ReAct implementation from scratch provides the foundation for building any agentic system, whether using frameworks or custom solutions, with complete transparency into the underlying mechanics.

---

**üîó Resources:**
- [ReAct Paper](https://arxiv.org/abs/2210.03629) - Original research by Yao et al.
- [LiteLLM Documentation](https://litellm.ai/) - Multi-provider LLM integration
- [Daily Dose of Data Science](https://www.dailydoseofds.com/)

**üìù Article Series:**
- ‚úÖ Part 1-9: Framework-based agent development with CrewAI
- ‚úÖ Part 10: ReAct Pattern Implementation From Scratch (This article)
- üîú Part 11: Advanced Agentic Design Patterns
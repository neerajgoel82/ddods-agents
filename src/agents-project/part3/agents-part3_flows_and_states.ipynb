{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Agents Crash Course - Part 3 Implementation\n",
    "\n",
    "This notebook demonstrates the key concepts from the Daily Dose of Data Science AI Agents article by Avi Chawla and Akshay Pachaar.\n",
    "\n",
    "## What you'll learn:\n",
    "1. **CrewAI Flows**, a powerful feature that enables the creation of structured, event-driven workflows for AI agents. This article focuses on moving beyond basic agent configurations to building complex, production-ready systems that seamlessly integrate deterministic processes with AI-driven autonomy through flow control mechanisms.\n",
    "\n",
    "## Prerequisites:\n",
    "- Install required packages\n",
    "- Set up API keys\n",
    "- Choose your LLM provider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Installation and Setup\n",
    "\n",
    "First, let's install the required packages and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this cell first)\n",
    "#!pip install crewai crewai-tools python-dotenv pyyaml IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# CrewAI imports\n",
    "from crewai import Agent, Task, Crew, Process, LLM\n",
    "from crewai_tools import SerperDevTool, FileReadTool\n",
    "\n",
    "print(\"âœ… All packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”‘ Environment Configuration\n",
    "\n",
    "Set up your environment variables. You have two options:\n",
    "\n",
    "### Option 1: Create a `.env` file with:\n",
    "```\n",
    "SERPER_API_KEY=\"your-serper-api-key\"\n",
    "AZURE_OPENAI_API_KEY=\"your-azure-openai-key\"\n",
    "AZURE_OPENAI_ENDPOINT=\"your-azure-openai-endpoint\"\n",
    "AZURE_OPENAI_API_VERSION=\"your-azure-openai-api-version\"\n",
    "AZURE_OPENAI_MODEL_NAME=\"your-azure-openai-model-name\"\n",
    "```\n",
    "\n",
    "### Option 2: Set them directly in this notebook (not recommended for production):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Option 2: Uncomment and set your API keys directly (not recommended for production)\n",
    "# os.environ['OPENAI_API_KEY'] = 'your-openai-api-key-here'\n",
    "# os.environ['SERPER_API_KEY'] = 'your-serper-api-key-here'\n",
    "\n",
    "# Configure LLM - Choose one of the options below:\n",
    "\n",
    "# Option A: Local Ollama (as mentioned in the article)\n",
    "#llm = LLM(\n",
    "#    model=\"ollama/llama3.2:1b\",\n",
    "#    base_url=\"http://localhost:11434\"\n",
    "#)\n",
    "\n",
    "# Option B: OpenAI GPT-4 (uncomment to use)\n",
    "# llm = LLM(model=\"gpt-4\")\n",
    "\n",
    "# Option C: OpenAI GPT-3.5-turbo (cheaper alternative)\n",
    "# llm = LLM(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Option D: Azure OpenAI\n",
    "openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "openai_model_name = os.getenv(\"AZURE_OPENAI_MODEL_NAME\")\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"azure/gpt-4o-mini\",\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_endpoint,\n",
    "    api_version=openai_api_version,\n",
    "    azure=True\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Environment configured!\")\n",
    "print(f\"LLM Model: {llm.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤– Example 1: Basic Flow\n",
    "\n",
    "In this walkthrough, weâ€™ll explore how CrewAI Flows allow you to effortlessly manage sequences of tasks powered by AI.\n",
    "\n",
    "To keep things practical, we'll build a simple yet interesting scenario: first generating a random movie genre, then using that genre to suggest a popular movie recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from crewai.flow.flow import Flow, start, listen\n",
    "\n",
    "openai_client = openai.AzureOpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    api_version=openai_api_version,\n",
    "    azure_endpoint=openai_endpoint\n",
    ")\n",
    "\n",
    "class MovieRecommendationFlow(Flow):\n",
    "\n",
    "    @start()\n",
    "    def generate_genre(self):\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Give me a random movie genre.\",\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        random_genre = response.choices[0].message.content.strip()\n",
    "        self.state[\"genre\"] = random_genre\n",
    "        return random_genre\n",
    "    \n",
    "\n",
    "    @listen(generate_genre)\n",
    "    def recommend_movie(self, random_genre):\n",
    "    \n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Recommend a movie in {random_genre} genre.\",\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        movie_recommendation = response.choices[0].message.content.strip()\n",
    "        self.state[\"recommendation\"] = movie_recommendation\n",
    "        return movie_recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = MovieRecommendationFlow()\n",
    "final_result = await flow.kickoff_async()\n",
    "\n",
    "print(f\"\\nMovie Recommendation: {final_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(flow.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is a basic demo that could have been done with a simple LLM call, the example was used to explain that this particular design pattern is incredibly powerful for several reasons:\n",
    "\n",
    "Flows automate AI-driven sequential task execution, removing manual intervention.\n",
    "Tasks can easily share data, ensuring consistent context management.\n",
    "You can effortlessly add more tasks and dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: States in Flows\n",
    "Managing the state effectively is essential for building robust and maintainable AI workflows. CrewAI Flows provides two approaches for handling state:\n",
    "\n",
    "Unstructured state management, which is more flexible and dynamic.\n",
    "Structured state management, which enforces schema validation using a model-based approach.\n",
    "In this guide, we'll go beyond just storing intermediate values and explore how to modify, update, and manage the state effectively throughout a flow.\n",
    "\n",
    "Instead of generating movie recommendations, this time weâ€™ll build a task management system where an AI agent:\n",
    "\n",
    "Generates a task for a software engineer.\n",
    "Updates the taskâ€™s status to \"In Progress.\"\n",
    "Marks the task as \"Completed.\"\n",
    "\n",
    "\n",
    "### Unstructured state\n",
    "In unstructured state management, all state attributes are stored dynamically in self.state, similar to how a Python dictionary works. This allows the flow to store and modify state values on the fly, without requiring a predefined schema\n",
    "\n",
    "Hereâ€™s how we can implement a task management system using an unstructured state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai.flow.flow import Flow, listen, start\n",
    "\n",
    "class TaskManagementFlow(Flow):\n",
    "\n",
    "    @start()\n",
    "    def generate_task(self):\n",
    "        print(f\"Flow started. State ID: {self.state['id']}\")\n",
    "        \n",
    "        # Step 1: Generate a new task\n",
    "        self.state[\"task\"] = \"Fix a critical bug in the payment system\"\n",
    "        self.state[\"status\"] = \"Pending\"\n",
    "        print(f\"Task generated: {self.state['task']} (Status: {self.state['status']})\")\n",
    "\n",
    "    @listen(generate_task)\n",
    "    def start_task(self):\n",
    "        # Step 2: Update task status to 'In Progress'\n",
    "        self.state[\"status\"] = \"In Progress\"\n",
    "        print(f\"Task status updated: {self.state['status']}\")\n",
    "\n",
    "    @listen(start_task)\n",
    "    def complete_task(self):\n",
    "        # Step 3: Mark task as 'Completed'\n",
    "        self.state[\"status\"] = \"Completed\"\n",
    "        print(f\"Task status updated: {self.state['status']}\")\n",
    "        print(f\"Final Task State: {self.state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the flow\n",
    "flow = TaskManagementFlow()\n",
    "final_result = await flow.kickoff_async()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured state\n",
    "While unstructured state management is flexible, structured state management enforces a predefined schema, ensuring that all state values conform to a specific format. This prevents accidental errors and makes debugging easier.\n",
    "\n",
    "Like we learned in Part 2 at the time of structured outputs, weâ€™ll use Pydantic to define a strict schema for the task stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "# Defining a structured state model\n",
    "class TaskState(BaseModel):\n",
    "    task: str = \"None\"\n",
    "    status: str = \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai.flow.flow import Flow, listen, start\n",
    "\n",
    "class StructuredTaskFlow(Flow[TaskState]):\n",
    "\n",
    "    @start()\n",
    "    def generate_task(self):\n",
    "        print(f\"Flow started. State ID: {self.state.id}\")\n",
    "        self.state.task = \"Develop a new API endpoint\"\n",
    "        self.state.status = \"Pending\"\n",
    "        print(f\"Task generated: {self.state.task} (Status: {self.state.status})\")\n",
    "\n",
    "    @listen(generate_task)\n",
    "    def start_task(self):\n",
    "        self.state.status = \"In Progress\"\n",
    "        print(f\"Task status updated: {self.state.status}\")\n",
    "\n",
    "    @listen(start_task)\n",
    "    def complete_task(self):\n",
    "        self.state.status = \"Completed\"\n",
    "        print(f\"Task status updated: {self.state.status}\")\n",
    "        print(f\"Final Task State: {self.state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the flow\n",
    "flow = StructuredTaskFlow()\n",
    "final_result = await flow.kickoff_async()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(flow.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Flow Control\n",
    "Managing the flow of tasks dynamically is a crucial part of AI-powered workflows. CrewAI Flows provides powerful tools to control execution paths based on conditions.\n",
    "\n",
    "Below, we shall cover three important mechanisms for flow control:\n",
    "\n",
    "- **or_ function**: Triggers a task when at least one of multiple conditions is met.\n",
    "- **and_ function**: Triggers a task only when all specified conditions are met.\n",
    "- **@router() decorator**: Directs the flow dynamically based on decision logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR conditional logic\n",
    "The or_ function allows a method to be executed when any one of multiple tasks is completed. This is useful when a task should proceed regardless of which event happens first.\n",
    "\n",
    "Letâ€™s build a system where a support request can come from two sources:\n",
    "\n",
    "Live chat\n",
    "Email ticket\n",
    "If a request is received from either channel, it should be logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai.flow.flow import Flow, listen, or_, start\n",
    "\n",
    "class SupportFlow(Flow):\n",
    "\n",
    "    @start()\n",
    "    def live_chat_request(self):\n",
    "        return \"Support request received via live chat\"\n",
    "\n",
    "    @start()\n",
    "    def email_ticket_request(self):\n",
    "        return \"Support request received via email ticket\"\n",
    "\n",
    "    @listen(or_(live_chat_request, email_ticket_request))\n",
    "    def log_request(self, request_source):\n",
    "        print(f\"Logging request: {request_source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the flow\n",
    "flow = SupportFlow()\n",
    "final_result = await flow.kickoff_async()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AND conditional logic\n",
    "The and_ function ensures that a task only executes when multiple conditions are satisfied. For instance, we can escalate a support ticket only after user response and agent review.\n",
    "\n",
    "In this example, a ticket should only be escalated if:\n",
    "\n",
    "The user confirms they still need help.\n",
    "A support agent has reviewed the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai.flow.flow import Flow, and_, listen, start\n",
    "\n",
    "class TicketEscalationFlow(Flow):\n",
    "\n",
    "    @start()\n",
    "    def user_confirms_issue(self):\n",
    "        self.state[\"user_confirmation\"] = True\n",
    "        print(\"User confirmed they still need assistance.\")\n",
    "\n",
    "    @listen(user_confirms_issue)\n",
    "    def agent_reviews_ticket(self):\n",
    "        self.state[\"agent_review\"] = True\n",
    "        print(\"Support agent has reviewed the ticket.\")\n",
    "\n",
    "    @listen(and_(user_confirms_issue, agent_reviews_ticket))\n",
    "    def escalate_ticket(self):\n",
    "        print(\"Escalating ticket to Level 2 support!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the flow\n",
    "flow = TicketEscalationFlow()\n",
    "final_result = await flow.kickoff_async()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Router logic\n",
    "The @router() decorator allows a method to decide the next task based on a condition. This is useful when different cases require different handling strategies. For instance, some logic (which could be AI-driven) can determine if a support request is urgent or non-urgent:\n",
    "\n",
    "Urgent tickets should be assigned to a live support agent.\n",
    "Non-urgent tickets should be sent to email support.\n",
    "The key concepts required in this workflow will be:\n",
    "\n",
    "Pydantic for structured state management\n",
    "Dynamic ticket classification.\n",
    "Routing execution flow using @router()\n",
    "Listening to specific values using @listen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class TicketState(BaseModel):\n",
    "    priority: str = \"low\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai.flow.flow import Flow, listen, router, start\n",
    "import random\n",
    "\n",
    "class TicketRoutingFlow(Flow[TicketState]):\n",
    "    @start()\n",
    "    def classify_ticket(self):\n",
    "        print(\"Classifying support ticket...\")\n",
    "        self.state.priority = random.choice([\"high\", \"low\"])\n",
    "        print(f\"Ticket classified as: {self.state.priority}\")\n",
    "\n",
    "    @router(classify_ticket)\n",
    "    def route_ticket(self):\n",
    "        if self.state.priority == \"high\":\n",
    "            return \"urgent_support\"\n",
    "        else:\n",
    "            return \"email_support\"\n",
    "        \n",
    "    @listen(\"urgent_support\")\n",
    "    def assign_to_agent(self):\n",
    "        print(\"Urgent ticket assigned to a live support agent!\")\n",
    "\n",
    "    @listen(\"email_support\")\n",
    "    def send_to_email_queue(self):\n",
    "        print(\"Non-urgent ticket sent to email support queue.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the flow\n",
    "flow = TicketRoutingFlow()\n",
    "final_result = await flow.kickoff_async()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddods-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

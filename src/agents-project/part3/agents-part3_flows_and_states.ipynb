{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Agents Crash Course - Part 3 Implementation\n",
    "\n",
    "This notebook demonstrates the key concepts from the Daily Dose of Data Science AI Agents article by Avi Chawla and Akshay Pachaar.\n",
    "\n",
    "## What you'll learn:\n",
    "1. **CrewAI Flows**, a powerful feature that enables the creation of structured, event-driven workflows for AI agents. This article focuses on moving beyond basic agent configurations to building complex, production-ready systems that seamlessly integrate deterministic processes with AI-driven autonomy through flow control mechanisms.\n",
    "\n",
    "## Prerequisites:\n",
    "- Install required packages\n",
    "- Set up API keys\n",
    "- Choose your LLM provider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Installation and Setup\n",
    "\n",
    "First, let's install the required packages and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this cell first)\n",
    "#!pip install crewai crewai-tools python-dotenv pyyaml IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# CrewAI imports\n",
    "from crewai import Agent, Task, Crew, Process, LLM\n",
    "from crewai_tools import SerperDevTool, FileReadTool\n",
    "\n",
    "print(\"âœ… All packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”‘ Environment Configuration\n",
    "\n",
    "Set up your environment variables. You have two options:\n",
    "\n",
    "### Option 1: Create a `.env` file with:\n",
    "```\n",
    "SERPER_API_KEY=\"your-serper-api-key\"\n",
    "AZURE_OPENAI_API_KEY=\"your-azure-openai-key\"\n",
    "AZURE_OPENAI_ENDPOINT=\"your-azure-openai-endpoint\"\n",
    "AZURE_OPENAI_API_VERSION=\"your-azure-openai-api-version\"\n",
    "AZURE_OPENAI_MODEL_NAME=\"your-azure-openai-model-name\"\n",
    "```\n",
    "\n",
    "### Option 2: Set them directly in this notebook (not recommended for production):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Option 2: Uncomment and set your API keys directly (not recommended for production)\n",
    "# os.environ['OPENAI_API_KEY'] = 'your-openai-api-key-here'\n",
    "# os.environ['SERPER_API_KEY'] = 'your-serper-api-key-here'\n",
    "\n",
    "# Configure LLM - Choose one of the options below:\n",
    "\n",
    "# Option A: Local Ollama (as mentioned in the article)\n",
    "#llm = LLM(\n",
    "#    model=\"ollama/llama3.2:1b\",\n",
    "#    base_url=\"http://localhost:11434\"\n",
    "#)\n",
    "\n",
    "# Option B: OpenAI GPT-4 (uncomment to use)\n",
    "# llm = LLM(model=\"gpt-4\")\n",
    "\n",
    "# Option C: OpenAI GPT-3.5-turbo (cheaper alternative)\n",
    "# llm = LLM(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Option D: Azure OpenAI\n",
    "openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "openai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "openai_model_name = os.getenv(\"AZURE_OPENAI_MODEL_NAME\")\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"azure/gpt-4o-mini\",\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_endpoint,\n",
    "    api_version=openai_api_version,\n",
    "    azure=True\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Environment configured!\")\n",
    "print(f\"LLM Model: {llm.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤– Example 1: Basic Flow\n",
    "\n",
    "In this walkthrough, weâ€™ll explore how CrewAI Flows allow you to effortlessly manage sequences of tasks powered by AI.\n",
    "\n",
    "To keep things practical, we'll build a simple yet interesting scenario: first generating a random movie genre, then using that genre to suggest a popular movie recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from crewai.flow.flow import Flow, start, listen\n",
    "\n",
    "openai_client = openai.AzureOpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    api_version=openai_api_version,\n",
    "    azure_endpoint=openai_endpoint\n",
    ")\n",
    "\n",
    "class MovieRecommendationFlow(Flow):\n",
    "\n",
    "    @start()\n",
    "    def generate_genre(self):\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Give me a random movie genre.\",\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        random_genre = response.choices[0].message.content.strip()\n",
    "\n",
    "        return random_genre\n",
    "    \n",
    "\n",
    "    @listen(generate_genre)\n",
    "    def recommend_movie(self, random_genre):\n",
    "    \n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Recommend a movie in {random_genre} genre.\",\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        movie_recommendation = response.choices[0].message.content.strip()\n",
    "        \n",
    "        return movie_recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = MovieRecommendationFlow()\n",
    "final_result = await flow.kickoff_async()\n",
    "\n",
    "print(f\"\\nMovie Recommendation: {final_result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is a basic demo that could have been done with a simple LLM call, the example was used to explain that this particular design pattern is incredibly powerful for several reasons:\n",
    "\n",
    "Flows automate AI-driven sequential task execution, removing manual intervention.\n",
    "Tasks can easily share data, ensuring consistent context management.\n",
    "You can effortlessly add more tasks and dependencies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddods-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
